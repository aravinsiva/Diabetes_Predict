#!/usr/bin/env python
# coding: utf-8

# In[1]:


import keras
import sys
import numpy as np
import sklearn
import pandas as pd



# In[2]:


#IMPORT THE DIABETES DATASET
url= 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv'
names = ['n_preganant','glucose_concentration','blood_pressure(mm Hg)', 
         'skin_thickness','serum_insulin(mu U/ml)','BMI','pedrigree_function',
        'age','class']

df= pd.read_csv(url,names=names)


# In[3]:


df.describe()


# In[4]:


#BEGIN DATA CLEANING AND PREPROCESSING 
df[df['glucose_concentration']==0]


# In[5]:


#drop certain zero values
columns = ['glucose_concentration','blood_pressure(mm Hg)', 
         'skin_thickness','serum_insulin(mu U/ml)','BMI']

for col in columns:
    df[col].replace(0,np.NaN,inplace=True)
    
df.describe()


# In[6]:


#drop rows with missing values
df.dropna(inplace=True)

#summarize data after preprcessing
df.describe()


# In[7]:


# Convert dataframe into a numpy array
dataset= df.values
print (dataset )
print ('\n\n')
print (dataset.shape)


# In[8]:


#split into input and an output
X= dataset[:,0:8]
Y= dataset[:,8].astype(int)




# In[9]:


print(X.shape)
print(Y.shape)
print (X[:5])


# In[10]:


#Sample output
#Zero demonstartes a healthy individual
#One indicates and indivdual who has diabetes
print(Y[:5])


# In[11]:


#Normalize the dataset using sklearn StandardScaler
from sklearn.preprocessing import StandardScaler

scaler=StandardScaler().fit(X)
print(scaler)


# In[12]:


#Transform and display training data
X_standardized= scaler.transform(X)

data= pd.DataFrame(X_standardized)
data.describe()


# In[13]:


from sklearn.model_selection import GridSearchCV, KFold
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import Adam


# In[14]:


#Define model to be used

def create_model():
    model= Sequential()
    model.add(Dense(8, input_dim = 8, kernel_initializer= 'normal', 
                    activation= 'relu'))
    model.add(Dense(4, input_dim= 8,kernel_initializer= 'normal', 
                    activation= 'relu'))
    model.add(Dense(1,activation='sigmoid'))
    
    #compile the model
    adam = Adam(lr= 0.01)
    model.compile(loss= 'binary_crossentropy', optimizer= adam , 
                 metrics= ['accuracy'])
    
    return model


# In[15]:


#check model
model= create_model()
print(model.summary())


# In[16]:


seed= 6
np.random.seed(seed)

model = KerasClassifier(build_fn= create_model, verbose=0)

#Define grid search parameter
batch_size= [10,20,40]

epochs = [10,50,100]

#need a dictionary of the grid search

param_grid= dict(batch_size=batch_size, epochs=epochs)

#build and fitGridSearch

gird= GridSearchCV(estimator = model, param_grid= param_grid, 
                  cv= KFold(random_state=seed), verbose = 10)
grid_results= gird.fit(X_standardized, Y)


#Summarize results of the network

print('Best: {0}, using {1}'.format(grid_results.best_score_,
                                   grid_results.best_params_))
means= grid_results.cv_results_['mean_test_score']
stds= grid_results.cv_results_['std_test_score']
params = grid_results.cv_results_['params']


for mean, stdev, param in zip(means, stds, params):
    print(mean, stdev, param)



# In[17]:


# Do a grid search for learning rate and dropout rate
# import necessary packages
from keras.layers import Dropout

# Define a random seed
seed = 6
np.random.seed(seed)

# Start defining the model
def create_model(learn_rate, dropout_rate):
    # create model
    model = Sequential()
    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    
    # compile the model
    adam = Adam(lr = learn_rate)
    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])
    return model

# create the model
model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)

# define the grid search parameters
learn_rate = [0.001, 0.01, 0.1]
dropout_rate = [0.0, 0.1, 0.2]

# make a dictionary of the grid search parameters
param_grid = dict(learn_rate=learn_rate, dropout_rate=dropout_rate)

# build and fit the GridSearchCV
grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)
grid_results = grid.fit(X_standardized, Y)

# summarize the results
print("Best: {0}, using {1}".format(grid_results.best_score_, grid_results.best_params_))
means = grid_results.cv_results_['mean_test_score']
stds = grid_results.cv_results_['std_test_score']
params = grid_results.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print('{0} ({1}) with: {2}'.format(mean, stdev, param))


# In[24]:


#SOME NOTES SO FAR
#-------------------------------------------------------------->

#Percentage difference between accuracies above is too large
#Continued optimization of the model
#Optimization can be done by altering the learn rate and the drop out rate 
#use the same skeleton and play around with some of the values 
#This is in order to help optimize the alogorithm



#From the above function we know that the most accurate readings were
#when the value of batch_size was 20 and the epochs were 100
#Therefore in our optimization we will stick to these values


#define the grid search parameters
#learn rate controls the speed at which the parameters of the network 
#need a dictionary of the grid search
#Will determine the extent of change to the weights at each step
#If the changes are too large the algorithm will not fine the local mimimum
#in accuracy
#Should the learning rate be too small the parameters may not be 
#updated enough in order to satisfy the acceptance criteria

#As obdserved from above the ideal way to optimize the neural network is
#to use a learn rate of 0.01 and no dropout rate
#This in turn also prevents overfitting (becoming too fit to the 
#training data)


# In[18]:


#How to further improve the alogorithm?
#Look to improve kernel_initializer, activation and neurons


# Do a grid search for learning rate and dropout rate
# import necessary packages
from keras.layers import Dropout

# Define a random seed
seed = 6
np.random.seed(seed)

# Start defining the model
#define model to take activation andl init 
#remove dropoup out rates
def create_model(activation, init):
    # create model
    model = Sequential()
    model.add(Dense(8, input_dim = 8, kernel_initializer=init, activation=activation))
    model.add(Dense(4, input_dim = 8, kernel_initializer=init, activation=activation))
    model.add(Dense(1, activation='sigmoid'))
    
    # compile the model
    #Hardcode the value of the leatrning rate to the ideal value
    adam = Adam(lr = 0.001)
    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])
    return model

# create the model
model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)

#define grid search parameters
activation= ['softmax','relu','tanh','linear']
init= ['uniform','normal','zero']


# make a dictionary of the grid search parameters
param_grid = dict(activation=activation, init=init)

# build and fit the GridSearchCV
grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)
grid_results = grid.fit(X_standardized, Y)

# summarize the results
print("Best: {0}, using {1}".format(grid_results.best_score_, grid_results.best_params_))
means = grid_results.cv_results_['mean_test_score']
stds = grid_results.cv_results_['std_test_score']
params = grid_results.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print('{0} ({1}) with: {2}'.format(mean, stdev, param))


# In[20]:


#Begin tunning the ideal number of neurons to be used

#How to further improve the alogorithm?
#Look to improve kernel_initializer, activation and neurons


# Do a grid search for learning rate and dropout rate
# import necessary packages
from keras.layers import Dropout

# Define a random seed
seed = 6
np.random.seed(seed)

# Start defining the model
#define model to take activation andl init 
#remove dropoup out rates
def create_model(neuron1,neuron2):
    # create model
    model = Sequential()
    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='tanh'))
    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='tanh'))
    model.add(Dense(1, activation='sigmoid'))
    
    # compile the model
    #Hardcode the value of the leatrning rate to the ideal value
    adam = Adam(lr = 0.001)
    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])
    return model

# create the model
model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)

#define grid search parameters
neuron1= [4,8,16]
neuron2 = [2,4,8]

# make a dictionary of the grid search parameters
param_grid = dict(neuron1= neuron1, neuron2=neuron2)

# build and fit the GridSearchCV
grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed),refit=True, verbose = 10)
grid_results = grid.fit(X_standardized, Y)

# summarize the results
print("Best: {0}, using {1}".format(grid_results.best_score_, grid_results.best_params_))
means = grid_results.cv_results_['mean_test_score']
stds = grid_results.cv_results_['std_test_score']
params = grid_results.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print('{0} ({1}) with: {2}'.format(mean, stdev, param))


# In[21]:


#Best neuron quantity as seen above is 16 neurons for the first layer
#And 4 neurons for the  
y_pred= grid.predict(X_standardized)


# In[22]:


print(y_pred.shape)


# In[23]:


print (y_pred[:5])


# In[25]:


#Create a classification report and an accuracy score 
#Using the testing data

from sklearn.metrics import classification_report, accuracy_score

print (accuracy_score(Y,y_pred))
print (classification_report(Y, y_pred))


# In[26]:


#All of the above data is extremely relavnet in the medical field
#We can see false positives
#We can see we also have false negatives

#--------------------------------------------------------->

#Lets try and predict a sample patients diagnosis 

example = df.iloc[1]
print (example)



# In[34]:


#make prediction using otimized deep neural network
prediction= grid.predict(X_standardized[1].reshape(1,-1))

if (prediction==1):
    print ('Prediction: Patient has diabetes')
    
else:
    print ('Predition: Patient is healthy')
    
if (example['class']==1):
    print ('Actual: Patient has diabetes')
    
else:
    print ('Actual: Patient is healthy')
    
    

if (prediction==example['class']):
    print('SUCCESSFUL PREDICTION!!!!')


# In[ ]:




